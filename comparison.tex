In this section the five methods we have explained will be compared in term of performances on different downstream tasks. In addition two other methods which have been explored during the lectures will be considered, SimCLR \cite{chen2020simple} and BYOL \cite{grill2020bootstrap}. As we already said in the previous sections, the way in which we evaluate SSL methods is by evaluating how downstream tasks perform after the SSL phase. Of course the evaluation on the downstream task might depends by many factors: the chosen dataset, the network used for the encoding, the hyper-parameters used for the methods the type of augmentations applied and so on. In this section I tried to use the results taken from the papers of the considered methods, from survey \cite{technologies9010002} and from this paper \cite{ericsson2021well}. In order to have a fair comparison I tried to collect results when the methods have been compared with the same encoder, and I also compared the methods on different datasets.

\noindent In table \ref{tab:imagenet-top1-5-acc-comp} we report the top-1 and top-5 accuracy of the considered methods when tested for classification on the ImageNet dataset and also the performances of a supervised model trained directly without SSL.  All the methods uses as feature extractor a ResNet50 which is frozen after the SSL phase, and for the classification we only train a linear classifier which uses as inputs the representations of the frozen ResNet50 (sadly for some model we don't have the top-5 accuracy). In term of performances PIRL and CPC are the two worst models, while CMC does slightly better. SWAV and BYOL are by far the two best performing models, even if they do not reach the top-1 accuracy of the supervised model. MoCo and SimCLR are quite better than PIRL, CPC and CMP, but not as good as BYOL and SWAV.
\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|cc|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Method}} & \textbf{Architecture} & \multicolumn{2}{c|}{\textbf{ImageNet}} \\
		\multicolumn{1}{|c|}{} &  & Top1 & Top5 \\
		\hline
		Supervised & ResNet50 & 76.5 & - \\
		\hline
		PIRL & ResNet50 & 63.6 & - \\
		CPCv2 & ResNet50 & 63.8 & 85.3 \\
		CMC & ResNet50  & 66.2 & 87 \\
		SimCLR & ResNet50 & 69.3 & 89.0 \\
		MoCov2 & ResNet50 & 71.1 & - \\
		BYOL & ResNet50  & 74.3 & 91.6 \\ 
		SwAV & ResNet50 & 75.3 & - \\
		\hline
\end{tabular}
	\caption{Accuracy of the SSL methods using linear probing for fine-tuning}
	\label{tab:imagenet-top1-5-acc-comp}
\end{table}
\noindent Table \ref{tab:imagenet-1-perc-semisup} shows the performances of the methods in a semi-supervised learning scenario. Basically after having performed the SSL phase we fine-tune the models using in one case the 1\% and in the other case the 10\% of the dataset. We report also different versions of the same model but using a different encoders. If we only look at the models that use the ResNet50 the raking of the model is the same that we obtained in the case of the linear probing. Here we can also see that the network used for the encoding has of course a big impact on the performances, for instance CPCv2 with a ResNet161 encoder reaches an higher top-5 accuracy than SWAV and BYOL with a ResNet50.
\begin{table}[H]
	\centering
	\begin{tabular}{|l|l|cc|cc|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Method}} & \textbf{Architecture} & \multicolumn{2}{c|}{\textbf{ImageNet 1\%}} & \multicolumn{2}{c|}{\textbf{ImageNet 10\%}} \\
		\multicolumn{1}{|c|}{} &  & Top1 & Top5 & Top1 & Top5  \\
		\hline
		Supervised & ResNet50 & 25.4 & 48.4 & 56.4 & 80.4 \\
		PIRL & ResNet50 & 30.7 & 57.2  & 60.4 & 83.8 \\
		SimCLR & ResNet50 & 48.3 & 75.5  & 65.6 & 87.8 \\
		BYOL & ResNet50  & 53.2 & 78.4  & 68.8 & 89.0 \\ 
		SwAV & ResNet50 & 53.9 & 78.5  & 70.2 & 89.9 \\
		CPCv2 & ResNet161 & - & 77.9 & - & 91.2 \\
		SimCLR & ResNet50 (4$\times$) & 63.0 & 85.8 & 74.4 & 92.6 \\
		BYOL & ResNet50 (2$\times$) & 71.2 & 89.5 & 77.7 & 93.7 \\
		\hline
	\end{tabular}
	\caption{Semi-supervised learning accuracy}
	\label{tab:imagenet-1-perc-semisup}
\end{table}
\noindent For both tables \ref{tab:imagenet-top1-5-acc-comp} and \ref{tab:imagenet-1-perc-semisup} the results are taken from the original papers of the methods, so the training conditions might be different.\\
In tables \ref{tab:classification-multi-dataset-linear} and \ref{tab:classification-multi-dataset-finetune} we report the accuracy for image classification task of the considered methods on multiple datasets using the same encoder for every method (a ResNet50). In table \ref{tab:classification-multi-dataset-linear} the pre-trained feature extractor is kept frozen and the classification is performed using multinomial logistic regression, while in table \ref{tab:classification-multi-dataset-finetune} the whole architecture is fine-tuned with SGD, adding a MLP at the end of the encoder. In the right-most column we report the average accuracy across all the datasets. 
\begin{table}[H]
	\centering
	\scalebox{0.6}{
	\begin{tabular}{l|cccccccccccc|c|}
		\cline{2-14}
		& \multicolumn{1}{c|}{\textbf{Imagenet}} & \multicolumn{1}{c|}{\textbf{Aircraft}} & \multicolumn{1}{c|}{\textbf{Calthech101}} & \multicolumn{1}{c|}{\textbf{Cars}} & \multicolumn{1}{c|}{\textbf{CIFAR10}} & \multicolumn{1}{c|}{\textbf{CIFAR100}} & \multicolumn{1}{c|}{\textbf{DTD}} & \multicolumn{1}{c|}{\textbf{Flowers}} & \multicolumn{1}{c|}{\textbf{Food}} & \multicolumn{1}{c|}{\textbf{Pets}} & \multicolumn{1}{c|}{\textbf{SUN387}} & \multicolumn{1}{c|}{\textbf{VOC2007}} & \multicolumn{1}{c|}{\textbf{Average}} \\ \hline
		\multicolumn{1}{|l|}{Supervised} & 77.20 & 43.59 & 90.18 & 44.92 & 91.42 & 73.90 & 72.23 & 89.93 & 69.49 & 91.45 & 60.49 & 83.60 & 73.75 \\ \hline
		\multicolumn{1}{|l|}{PIRL} & 61.70 & 37.08 & 74.48 & 28.72 & 82.53 & 61.26 & 68.99 & 83.60 & 64.65 & 71.36 & 53.89 & 76.61 & 63.92 \\ \cline{1-1}
		\multicolumn{1}{|l|}{SimCLR} & 69.30 & 44.90 & 90.05 & 43.73 & 91.18 & 72.73 & 74.20 & 90.87 & 67.47 & 83.33 & 59.21 & 80.77 & 72.59 \\ \cline{1-1}
		\multicolumn{1}{|l|}{MoCo-v2} & 71.10 & 41.79 & 87.92 & 39.31 & 92.28 & 74.90 & 73.88 & 90.07 & 68.95 & 83.30 & 60.32 & 82.69 & 72.31 \\ \cline{1-1}
		\multicolumn{1}{|l|}{BYOL} & 74.30 & 53.87 & 91.46 & 56.40 & 93.26 & 77.86 & 76.91 & 94.50 & 73.01 & 89.10 & 59.99 & 81.14 & 77.05 \\ \cline{1-1}
		\multicolumn{1}{|l|}{SWAV} & 75.30 & 54.04 & 90.84 & 54.06 & 93.99 & 79.58 & 77.02 & 94.62 & 76.62 & 87.60 & 65.58 & 83.68 & 77.97 \\ \hline
	\end{tabular}
	}
	\caption{Result after fine-tuning on 1\% of the data of ImageNet}
	\label{tab:classification-multi-dataset-linear}
\end{table}

\begin{table}[H]
	\centering
	\scalebox{0.6}{
	\begin{tabular}{l|ccccccccccc|c|}
		\cline{2-13}
		& \multicolumn{1}{c|}{\textbf{Aircraft}} & \multicolumn{1}{c|}{\textbf{Calthech101}} & \multicolumn{1}{c|}{\textbf{Cars}} & \multicolumn{1}{c|}{\textbf{CIFAR10}} & \multicolumn{1}{c|}{\textbf{CIFAR100}} & \multicolumn{1}{c|}{\textbf{DTD}} & \multicolumn{1}{c|}{\textbf{Flowers}} & \multicolumn{1}{c|}{\textbf{Food}} & \multicolumn{1}{c|}{\textbf{Pets}} & \multicolumn{1}{c|}{\textbf{SUN387}} & \multicolumn{1}{c|}{\textbf{VOC2007}} & \multicolumn{1}{c|}{\textbf{Average}} \\ \hline
		\multicolumn{1}{|l|}{Supervised} & 83.50 & 91.01 & 82.61 & 96.39 & 82.91 & 73.30 & 95.50 & 84.60 & 92.42 & 63.56 & 84.76 & 84.60 \\ \cline{1-1}
		\multicolumn{1}{|l|}{PIRL} & 72.68 & 70.83 & 61.02 & 92.23 & 66.48 & 64.26 & 89.81 & 74.96 & 76.26 & 50.38 & 69.90 & 71.71 \\ \cline{1-1}
		\multicolumn{1}{|l|}{SimCLR} & 81.06 & 90.35 & 83.78 & 97.07 & 84.53 & 71.54 & 93.75 & 82.40 & 84.10 & 63.31 & 82.58 & 83.13 \\ \cline{1-1}
		\multicolumn{1}{|l|}{MoCo-v2} & 79.87 & 84.38 & 75.20 & 96.45 & 71.33 & 69.47 & 94.35 & 76.78 & 78.80 & 55.77 & 71.71 & 77.74 \\ \cline{1-1}
		\multicolumn{1}{|l|}{BYOL} & 79.45 & 89.40 & 84.60 & 97.01 & 83.95 & 73.62 & 94.48 & 85.54 & 89.62 & 63.96 & 82.70 & 84.03 \\ \cline{1-1}
		\multicolumn{1}{|l|}{SWAV} & 83.08 & 89.85 & 86.76 & 96.78 & 84.37 & 75.16 & 95.46 & 87.22 & 89.05 & 66.24 & 84.66 & 85.33 \\ \hline
	\end{tabular}
	}
	\caption{Result after fine-tuning on 1\% of the data of ImageNet}
	\label{tab:classification-multi-dataset-finetune}
\end{table}
\noindent Even in these experiments in general SWAV performs slightly better than BYOL (even if in not all the datasets). SimCLR and MoCo have similar performances while PIRL performs significantly worse. In this cases for some datasets the performances of the best self-supervised datasets are even slightly better than those of the directly trained supervised models.\\
Sadly for other tasks aside image classification I couldn't find a single dataset on which all the methods have been tested. Table \ref{tab:object-detection} shows the results of PIRL, MoCO-v2 and SWAV for an object detection task on the VOC07+12 dataset using a Faster R-CNN. Again PIRL is the worst model, while for two of the three metrics MoCo-v2 is a bit better than SWAV. In table \ref{tab:semantic-segmentation} we report the performances of MoCo, SimCLR and BYOL in a semantic segmentation task on the VOC2012 dataset using again a Faster R-CNN. BYOL again is the best model and SimCLR does a bit better than MoCo.
\begin{table}[H]
	\begin{minipage}{.5\linewidth}
		\centering
		\begin{tabular}{l|ccc|}
			\cline{2-4} 
			& \multicolumn{1}{c|}{AP} & \multicolumn{1}{c|}{$\text{AP}_{50}$} & \multicolumn{1}{c|}{$\text{AP}_{75}$} \\ \hline
			\multicolumn{1}{|l|}{Supervised} & 53.5                   & 81.3                     & 58.8                     \\ \cline{1-1}
			\hline
			\multicolumn{1}{|l|}{PIRL}       & 54.0                  & 80.7                     & 59.7                     \\ \cline{1-1}
			\multicolumn{1}{|l|}{MoCo-v2}    & 57.4                   & 82.5                     & 64.0                     \\ \cline{1-1}
			\multicolumn{1}{|l|}{SWAV}       & 56.1                   & 82.6                     & 62.7                     \\ \cline{1-1}
			\hline
		\end{tabular}
		\caption{Object detection VOC dataset}
		\label{tab:object-detection}
	\end{minipage}%
	\begin{minipage}{.5\linewidth}
		\centering
		\begin{tabular}{l|cc|}
			\cline{2-3}
			& \multicolumn{1}{c|}{$\text{AP}_{50}$} & mIoU \\ \hline
			\multicolumn{1}{|l|}{Supervised} & \multicolumn{1}{c|}{74.4} & 74.4 \\ \hline
			\multicolumn{1}{|l|}{MoCo}       & \multicolumn{1}{c|}{74.9} & 72.5 \\ \cline{1-1}
			\multicolumn{1}{|l|}{SimCLR}     & \multicolumn{1}{c|}{75.2} & 75.2 \\ 
			\cline{1-1}
			\multicolumn{1}{|l|}{BYOL}       & \multicolumn{1}{c|}{77.5} & 76.3 \\ \hline
		\end{tabular}
		\caption{Image segmentation VOC dataset}
		\label{tab:semantic-segmentation}
	\end{minipage}
\end{table}