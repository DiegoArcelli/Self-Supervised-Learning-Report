Contrastive Multi-view Coding (CMC) \cite{tian2020contrastive} starts from the framework proposed in CPC paper and they adapt it to maximize the mutual information between different views of the same image, removing the prediction part and focusing on contrastive learning. Suppose we have two different views of a dataset $V_1$ and $V_2$, in the predictive learning setup we use a non-linear transformation to transform $v_1 \in V_1$ to $v_2 \in V_2$, passing through a latent variable $z$: first we use an encoder $f$ to compute $z = f(v_1)$ and then we use a decoder $g$ to compute $\hat{v}_2 = g(z)$, where $\hat{v}_2$ is the prediction of $v_2$ given $v_1$. The parameters of the encoder and of the decoder are then trained using an objective function that tries to bring $\hat{v}_2$ close to $v_2$, like the L1 or the L2 loss.

In contrastive learning instead we want to learn an embedding that separates samples from two different distributions. Given a dataset of $V_1$ and $V_2$ that consists of a collection of samples $\{v_1^i, v_2^i\}_{i=1}^N$ we consider positive pairs those which are sampled from the joint distribution $x \sim p(v_1, v_2)$ where $x= \{v_1^i, v_2^i\}$, while we consider negative samples those sampled from the product of marginals $y \sim p(v_1)p(v_2)$ where $y = \{v_1^i, v_2^j\}$. The goal is to learn a critic function $h_\theta$ which is trained to output high values for positive pairs and low values for negative pairs. In this way we can use a contrastive loss function that we can use to train the model to correctly select a single positive sample out of a set that contains $k$ negative samples:
%\[\mathcal{L}_{contrast} = -\mathbb{E}_S \Bigg[ \log \frac{h_\theta(x)}{h_\theta(x) + \sum_{i=1}^k h_\theta(y_i)}  \Bigg] \]
% In order to construct $S$ we fix on view and enumerate possible negatives and positives from the other view, and this allows us to rewrite the loss as:
\[\mathcal{L}^{V_1, V_2}_{contrast} = -\mathbb{E}_S \Bigg[ \log \frac{h_\theta(\{v_1^1, v_2^1 \})}{\sum_{j=1}^{k+1}h_\theta(\{v_1^1, v_2^j \})}  \Bigg] \]
where $\{v_1^1, v_2^1 \}$ is the positive pair and $\{v_1^1, v_2^j \}$ with $j > 1$ is the a negative pair. To extract the latent representations of $v_1$ and $v_2$ we use two encoders $f_{\theta_1}(\cdot)$ and $f_{\theta_2}(\cdot)$. We compute the latent representations $z_1 = f_{\theta_1}(v_1)$ and $z_2 = f_{\theta_1}(v_2)$ which we can use to compute the output of the critic as:
\[ h_\theta(\{v_1, v_2\}) = \text{exp}\Bigg(\frac{z_1^Tz_2}{\lVert z_1\rVert \lVert z_2\rVert}\cdot \frac{1}{\tau}\Bigg) \]
In the formulation of $\mathcal{L}^{V_1, V_2}_{contrast}$ the view $V_1$ is used as anchor view, while we enumerate samples from view $V_2$, but we can also do the opposite and use the following loss:
\[ \mathcal{L}(V_1, V_2) = \mathcal{L}^{V_1, V_2}_{contrast} + \mathcal{L}^{V_2, V_1}_{contrast} \]
In the paper the authors prove that the optimal critic $h^*_\theta$ is proportional to the density ratio between the joint distribution $p(z_1, z_2)$ and the product of the marginals $p(z_1)p(z_2)$:
\[ h^*_\theta(\{ v_1,v_2\}) \propto \frac{p(z_1, z_2)}{p(z_1)p(z_2)} \propto \frac{p(z_1|z_2)}{p(z_1)} \]
that is the point-wise mutual information and from this the same bound obtained for CPC get be proved:
\[I(z_i;z_j) \ge \log(k) - \mathcal{L}_{contrast} \]
hence minimizing the contrastive loss yields to the maximization of the mutual information between the latent representation of the different views. 

Then the authors proposed two different ways for extending the above defined framework to the usage of multiple views. If we have $M$ different views of a dataset $V_1, \dots, V_M$ in the core view formulation we select one view that we want to optimize, for instance $V_1$, and we build a pairwise representations between $V_1$ and all the other views:
\[ \mathcal{L}_C = \sum_{j=2}^{M} \mathcal{L}(V_1, V_j)\]
In the full graph formulation we instead consider all the possible pairs:
\[ \mathcal{L}_F = \sum_{j=1}^{M} \sum_{i=1}^{j-1} \mathcal{L}(V_i, V_j)\]
The full graph formulation is more computational expensive but it has the advantage that it allows to capture more information between different views.