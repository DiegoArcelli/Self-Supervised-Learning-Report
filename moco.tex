Momentum Contrast (MoCo) \cite{he2020momentum} uses a view of contrastive learning as dictionary look-up. If we consider and encoded query $q$ and a and encoded set of of keys of the dictionary $\{k_1, k_2, \dots\}$ and we assure that there exists a single key $k_+$ that matches with $q$, we can define a contrastive loss ass a function that is low when $q$ is similar to its positive key $k_+$ and dissimilar to all the other (negative) keys. Specifically in the paper they use the InfoNCE loss: 
\[ \mathcal{L}_q = - \log\Bigg( \frac{\exp(q^Tk_+/\tau)}{\sum_{i=0}^K \exp(q^T k_i/\tau)}\Bigg)\]
that can be used as an unsupervised objective function for training the encoder networks that represent the queries and the keys: $q = f_q(x^q)$ and $k = f_k(x^k)$. Under this dictionary lookup prospective, contrastive learning is a way for building a discrete dictionary on high dimensional continuous inputs. This dictionary is dynamic since the keys are randomly sampled and $f_k(\cdot)$ changes during training. 

To address the problem of finding good negative samples MoCo maintains a queue of mini-batches of data samples. In this way the encoded keys from the preceding mini-batches can be used in the loss: at each training step the current mini batch is enqueued in the dictionary and the oldest mini-batches is dequeued from the dictionary. The problem of using a large queue as a dictionary can be the update of the key encoder using back propagation intractable, since the gradient should propagate to all the samples in the queue, so the authors solve this problem by updating $f_k$'s parameters $\theta_k$ as a moving average of $f_q$'s parameters $\theta_q$:
\[\theta_k \leftarrow m\theta_k + (1-m)\theta_q \]
where $m \in [0,1)$. $\theta_q$ is normally updated using back-propagation. In this way $\theta_k$ evolves more smoothly than $\theta_q$. In this way even if the keys in the dictionary are encoded by different encoders, the difference between the encoders is now smaller.

MoCo has been improved in a successive work \cite{chen2020simple} in which the authors applied to it some of the improvement proposed by SimCLR, which are a stronger data augmentation and the use of a MLP projection head instead of a linear layer for the fine-tuning, and they call this improved version MoCo-v2.