Momentum Contrast (MoCo) \cite{he2020momentum} uses a prospective of contrastive learning as dictionary look-up. If we consider and encoded query $q$ and a and encoded set of of keys of the dictionary $\{k_1, k_2, \dots\}$ and we assume that there exists a single key $k_+$ that matches with $q$, we can define a contrastive loss ass a function that is low when $q$ is similar to its positive key $k_+$ and dissimilar to all the other negative keys. Specifically in the paper they use the InfoNCE loss: 
\[ \mathcal{L}_q = - \log\Bigg( \frac{\exp(q^Tk_+/\tau)}{\sum_{i=0}^K \exp(q^T k_i/\tau)}\Bigg)\]
that can be used as an unsupervised objective function for training the encoder networks that represent the queries and the keys: $q = f_q(x^q)$ and $k = f_k(x^k)$. Under this dictionary lookup prospective, contrastive learning is a way for building a discrete dictionary on high dimensional continuous inputs. This dictionary is dynamic since the keys are randomly sampled and $f_k(\cdot)$ changes during training.\\
Instead of using a memory bank like the ones of CMC and PIRL, MoCo maintains a queue of mini-batches of data samples. In this way the encoded keys from the preceding mini-batches can be used in the loss: at each training step the current mini batch is inserted in the queue and the oldest mini-batches is removed from the queue. The problem of using a large queue as a dictionary can be the update of the key encoder using back propagation intractable, since the gradient should propagate to all the samples in the queue, so the authors solve this problem by updating $f_k$'s parameters $\theta_k$ as a moving average of $f_q$'s parameters $\theta_q$:
\[\theta_k \leftarrow m\theta_k + (1-m)\theta_q \]
where $m \in [0,1)$. $\theta_q$ is normally updated using back-propagation. In this way $\theta_k$ evolves more smoothly than $\theta_q$, so even if the keys in the queue are encoded by different encoders, the difference between these encoders can be made small.\\
MoCo has been improved in a successive work \cite{chen2020simple} in which the authors applied to it some of the improvement proposed by SimCLR, which are a stronger data augmentation and the use of a MLP projection head after having computed the representation of the images using the encoder networks.