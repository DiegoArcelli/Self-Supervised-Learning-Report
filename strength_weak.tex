As we have seen from the performances comparison CPC, PIRL, CMC and MoCo perform much worse than the other methods. This makes sense since they were the first methods proposed and they have been improved by SimCLR, MoCov2, SWAV and BYOL. 

Among all the methods considered CPC was the first one to be proposed, and it is very different from all the other methods, since it don't use data augmentation and it uses the idea of creating a grid of embedding and train the auto-regressive model to predict future representation. CPC framework has been heavily simplified in CMC, removing the auto-regressive part, and introducing the idea of contrasting different views of the image. PIRL is similar to CMC the main difference is that CMC operates on different views and it learns a representation that is covariant to the image transformation while PIRL forces and invariant representation. Both PIRL and CMC used the memory bank for the negative mining, the main improvement made by MoCo was the idea of using instead of a memory bank a queue of the last $K$ mini-batches embedding. MoCo it has been then improved in MoCov2 taking from SimCLR the ideas of using more heavy data augmentation and the usage of the MLP projection head. SWAV merges the framework of contrastive learning with clustering based SSL, by using the 

One of the differences among the methods is the technique they use for negative mining. CPC didn't consider this problem at all, PIRL and CMC uses a large memory bank, MoCo uses the queue of the embedding of the last $K$ mini-batches, while SimCLR, SWAV and BYOL use large batch sizes.