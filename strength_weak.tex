The performances comparison shows that CPC, PIRL and CMC perform worse than the other methods, while the two best models are SWAV and BYOL. Moco-v2 and SimCLR are not as good as SWAV and BYOL but they are better than CPC, PIRL and CMC, and SimCLR performs a slightly better than MoCo-v2.\\
Among all the methods considered CPC was the first one to be proposed, and it is very different from all the other methods, since it uses the idea of creating a grid of embeddings and train the auto-regressive model to predict future representation of these embeddings. CPC framework has been heavily simplified in CMC, removing the auto-regressive part, and introducing the idea of contrasting different views of the image. Moreover in CMC the views are produced separating the image channels and not by applying transformation to the original image. PIRL seems similar to CMC but actually in CMC the networks used to encode the different views of the images are specific for each view, so they learn information specific to the views, while in PIRL contrastive learning is employed to learn a single embedding networks which produces representations invariant to the specific characteristic of the different views created with data augmentation. Both PIRL and CMC uses a memory bank to store samples to use in the contrastive loss. SimCLR is very similar to PIRL but instead of using the memory bank it uses a very large batch size during the training for the pretext task and it also apply the MLP projection to the image representation computed by the CNN. The main characteristic of MoCo is the usage of a queue of embeddings of the last $K$ mini-batches, instead of a classic a memory bank. MoCo has been improved in MoCo-v2 taking from SimCLR the ideas of using heavier data augmentation and the usage of the MLP projection head.\\
Both SWAV and BYOL differ from all the other methods since they use  predictive pretext tasks instead of contrastive learning. BYOL uses the idea of applying a classifier to the output of the online network to predict the output of the target network. In SWAV instead there is no distinction between target and online network, and the main characteristic is the usage of clustering to perform the swapped prediction problem. BYOL is more straightforward than SWAV, since in BYOL we simply compute the embeddings of the two inputs and we use the classifier to predict the embedding of the target network from the embedding of the online network, using the mean squared error as loss function, while in SWAV we also need to solve the optimization problem to compute the clusters assignment of the inputs.\\
One of the main difference among the methods is the technique they use to retrieve the negative samples to use in the loss. CPC didn't consider this problem at all, PIRL and CMC use a large memory bank, MoCo uses the queue of the embedding of the last $K$ mini-batches which is less space and time demanding than the memory bank, while SimCLR, SWAV and BYOL just use large batch sizes. Using large batch sizes instead of an explicit memory bank has of course the advantage that we spare time and computations for handling the memory, but we are limited by the memory size of the GPU. Since SWAV and BYOL doesn't use the idea of contrasting positive and negative pairs, they have the advantage that they don't need to use those techniques, and moreover they even reach better performances than the other methods.