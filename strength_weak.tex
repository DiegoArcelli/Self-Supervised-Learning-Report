As we have seen from the performances comparison CPC, PIRL and CMC perform much worse than the other methods. This makes sense since they were the first methods proposed and they have been improved by SimCLR, MoCov2, SWAV and BYOL. 

Among all the methods considered CPC was the first one to be proposed, and it is very different from all the other methods, since it uses the idea of creating a grid of embedding and train the auto-regressive model to predict future representation. CPC framework has been heavily simplified in CMC, removing the auto-regressive part, and introducing the idea of contrasting different views of the image. Moreover in CMC the views are produced separating the image channels and not by applying transformation to the original image. PIRL seems similar to CMC but actually in CMC the networks used to encode the different views of the images are specific for each view, so they learn information specific to the views, while in PIRL contrastive learning is employed to learn a single embedding networks which produces representations invariant to the specific characteristic of the different views created with data augmentation. Both PIRL and CMC used the memory bank to store negative samples. SimCLR is very similar to PIRL but instead of using the memory bank it uses a very large batch size during the pre-training and it also apply the MLP projection to the image representation computed by the CNN. The main improvement made by MoCo was the idea of using instead of a memory bank a queue of the last $K$ mini-batches embedding. MoCo it has been then improved in MoCov2 taking from SimCLR the ideas of using more heavy data augmentation and the usage of the MLP projection head.

Both SWAV and BYOL uses a predictive pretext task. BYOL uses the idea of applying a classifier to the output of the online network to predict the output of the target network instead of applying a contrastive loss. In SWAV instead there is no distinction between target and online network, and it uses the clusters as labels for the swapped prediction problems.

One of the differences among the methods is the technique they use for negative mining. CPC didn't consider this problem at all, PIRL and CMC uses a large memory bank, MoCo uses the queue of the embedding of the last $K$ mini-batches which is less space and time demanding than the memory bank, while SimCLR, SWAV and BYOL just use large batch sizes. Using large batch sizes instead of an explicit memory bank has of course the advantage that we spare time and computations for handling the memory, but we are limited by the memory size of the GPU.

Since SWAV and BYOL doesn't use the idea of contrasting positive and negative pairs, they have the advantage that they don't need specific techniques for negative mining.