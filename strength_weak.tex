As we have seen from the performances comparison CPC, PIRL, CMC and MoCo perform much worse than the other methods. This makes sense since they were the first methods proposed and they have been improved by SimCLR, MoCov2, SWAV and BYOL. 

Among all the methods considered CPC was the first one to be proposed, and it is very different from all the other methods, since it don't use data augmentation and it uses the idea of creating a grid of embedding and train the auto-regressive model to predict future representation. CPC framework has been heavily simplified in CMC, removing the auto-regressive part, and introducing the idea of contrasting different views of the image. Moreover in CMC the views are produced separating the image channels and not by applying transformation to the original image. PIRL seems similar to CMC but actually in CMC the networks used to encode the different views of the images are specific for each view, so they learn information specific to the view, while in PIRL contrastive learning is employed to learn a single embedding networks which produces representations invariant to the specific characteristic of the different views created with data augmentation. Both PIRL and CMC used the memory bank for the negative mining, the main improvement made by MoCo was the idea of using instead of a memory bank a queue of the last $K$ mini-batches embedding. MoCo it has been then improved in MoCov2 taking from SimCLR the ideas of using more heavy data augmentation and the usage of the MLP projection head. SWAV merges the framework of contrastive learning with clustering based SSL, by using the clusters as labels for the swapped prediction problems. Also BYOL add a prediction step in the contrastive learning framework since we train the online network to predict the input representation produced by the target network. So it's interesting to notice that the two best performing models are the ones which takes the classical contrastive learning framework and they add to it a prediction step, like in CPC. 

One of the differences among the methods is the technique they use for negative mining. CPC didn't consider this problem at all, PIRL and CMC uses a large memory bank, MoCo uses the queue of the embedding of the last $K$ mini-batches which is less space and time demanding than the memory bank, while SimCLR, SWAV and BYOL just use large batch sizes.